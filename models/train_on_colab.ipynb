{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# ML Footprint Prediction - GPU Training\n",
                "\n",
                "> Multi-output XGBoost training with Colab GPU (VS Code Local Runtime)\n",
                "\n",
                "This notebook runs locally in VS Code with a connected Colab GPU runtime.\n",
                "All files are accessed from your local filesystem - no uploads needed.\n",
                "\n",
                "---\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "1. Connect to Colab GPU: `Ctrl+Shift+P` -> \"Notebook: Select Notebook Kernel\" -> \"Connect to Google Colab\"\n",
                "2. Ensure you have the training data in `data/data_splitter/output/`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup"
            },
            "source": [
                "## Step 1: Verify GPU and Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import sys\n",
                "\n",
                "# Check GPU availability\n",
                "result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
                "if result.returncode == 0:\n",
                "    print(\"GPU Information:\")\n",
                "    print(result.stdout)\n",
                "else:\n",
                "    print(\"[WARNING] No GPU detected. Training will be slower on CPU.\")\n",
                "\n",
                "# Install XGBoost with GPU support if needed\n",
                "try:\n",
                "    import xgboost as xgb\n",
                "    print(f\"\\nXGBoost version: {xgb.__version__}\")\n",
                "except ImportError:\n",
                "    print(\"Installing XGBoost...\")\n",
                "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"xgboost\"])\n",
                "    import xgboost as xgb\n",
                "    print(f\"XGBoost version: {xgb.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "clone"
            },
            "source": [
                "## Step 2: Verify Local Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "clone_repo"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Get project root (parent of models/)\n",
                "PROJECT_ROOT = Path(__file__).parent.parent if '__file__' in dir() else Path.cwd().parent\n",
                "if PROJECT_ROOT.name == 'models':\n",
                "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
                "\n",
                "print(f\"Project root: {PROJECT_ROOT}\")\n",
                "\n",
                "# Define paths\n",
                "PATHS = {\n",
                "    'train': PROJECT_ROOT / 'data/data_splitter/output/train.csv',\n",
                "    'validate': PROJECT_ROOT / 'data/data_splitter/output/validate.csv',\n",
                "    'material_dataset': PROJECT_ROOT / 'data/data_calculations/input/material_dataset_final.csv',\n",
                "    'src': PROJECT_ROOT / 'models/src',\n",
                "    'save_dir': PROJECT_ROOT / 'models/saved/gpu_training',\n",
                "    'logs': PROJECT_ROOT / 'models/logs',\n",
                "}\n",
                "\n",
                "# Verify all required files exist\n",
                "print(\"\\nChecking required files:\")\n",
                "all_ok = True\n",
                "for name, path in PATHS.items():\n",
                "    exists = path.exists()\n",
                "    status = \"[OK]\" if exists else \"[MISSING]\"\n",
                "    if path.is_file() or name in ['train', 'validate', 'material_dataset']:\n",
                "        size_str = f\" ({path.stat().st_size / (1024*1024):.1f} MB)\" if exists and path.is_file() else \"\"\n",
                "        print(f\"  {status} {name}: {path}{size_str}\")\n",
                "    else:\n",
                "        print(f\"  {status} {name}: {path}\")\n",
                "    if not exists and name not in ['save_dir', 'logs']:\n",
                "        all_ok = False\n",
                "\n",
                "if all_ok:\n",
                "    print(\"\\n[SUCCESS] All required files found!\")\n",
                "else:\n",
                "    print(\"\\n[ERROR] Some files are missing. Check paths above.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "train_header"
            },
            "source": [
                "## Step 3: Load Data and Add Source to Path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train_full",
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "import sys\n",
                "\n",
                "# Add src to path for imports\n",
                "src_path = str(PATHS['src'])\n",
                "if src_path not in sys.path:\n",
                "    sys.path.insert(0, str(PATHS['src'].parent))  # Add models/ to path\n",
                "\n",
                "# Import project modules\n",
                "from src.data_loader import load_data, FEATURE_COLUMNS, TARGET_COLUMNS, MATERIAL_COLUMNS\n",
                "from src.formula_features import add_formula_features\n",
                "from src.preprocessor import FootprintPreprocessor\n",
                "from src.trainer import FootprintModelTrainer\n",
                "from src.evaluator import ModelEvaluator\n",
                "from src.config import BASELINE_CONFIG\n",
                "from src.utils import set_random_seed\n",
                "\n",
                "print(\"[OK] All modules imported successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "train_quick"
            },
            "source": [
                "## Step 4: Load and Prepare Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train_quick_exec"
            },
            "outputs": [],
            "source": [
                "# Set random seed for reproducibility\n",
                "set_random_seed(42)\n",
                "\n",
                "# Load data (use sample_size for quick testing, None for full dataset)\n",
                "SAMPLE_SIZE = None  # Set to e.g. 10000 for quick test, None for full ~676K samples\n",
                "\n",
                "print(f\"Loading data{f' (sample: {SAMPLE_SIZE})' if SAMPLE_SIZE else ' (full dataset)'}...\")\n",
                "X_train, y_train, X_val, y_val = load_data(\n",
                "    train_path=str(PATHS['train']),\n",
                "    val_path=str(PATHS['validate']),\n",
                "    sample_size=SAMPLE_SIZE\n",
                ")\n",
                "\n",
                "print(f\"\\nTraining set: {len(X_train):,} samples\")\n",
                "print(f\"Validation set: {len(X_val):,} samples\")\n",
                "print(f\"Features: {len(FEATURE_COLUMNS)}\")\n",
                "print(f\"Targets: {TARGET_COLUMNS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "results_header"
            },
            "source": [
                "## Step 5: Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "view_logs"
            },
            "outputs": [],
            "source": [
                "# Keep raw validation for robustness testing\n",
                "X_val_raw = X_val.copy()\n",
                "\n",
                "# Add formula-based features (physics-informed)\n",
                "print(\"Adding formula-based features...\")\n",
                "X_train = add_formula_features(X_train, MATERIAL_COLUMNS, str(PATHS['material_dataset']))\n",
                "X_val = add_formula_features(X_val, MATERIAL_COLUMNS, str(PATHS['material_dataset']))\n",
                "\n",
                "# Preprocess (encode categoricals, scale numericals)\n",
                "print(\"Preprocessing features...\")\n",
                "preprocessor = FootprintPreprocessor()\n",
                "X_train_processed = preprocessor.fit_transform(X_train)\n",
                "X_val_processed = preprocessor.transform(X_val)\n",
                "\n",
                "# Get final feature set\n",
                "feature_cols = preprocessor.get_feature_names()\n",
                "X_train_final = X_train_processed[feature_cols]\n",
                "X_val_final = X_val_processed[feature_cols]\n",
                "\n",
                "print(f\"\\n[OK] Feature engineering complete\")\n",
                "print(f\"Final feature count: {len(feature_cols)}\")\n",
                "print(f\"Training shape: {X_train_final.shape}\")\n",
                "print(f\"Validation shape: {X_val_final.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "view_metrics"
            },
            "outputs": [],
            "source": [
                "## Step 6: Train Model (GPU Accelerated)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "show_plots"
            },
            "outputs": [],
            "source": [
                "# Configure for GPU training\n",
                "config = BASELINE_CONFIG.copy()\n",
                "config['tree_method'] = 'gpu_hist'  # Use GPU acceleration\n",
                "config['device'] = 'cuda'\n",
                "\n",
                "print(\"Training Configuration:\")\n",
                "for key, value in config.items():\n",
                "    if key != 'random_state':\n",
                "        print(f\"  {key}: {value}\")\n",
                "\n",
                "# Initialize trainer\n",
                "trainer = FootprintModelTrainer(**config)\n",
                "\n",
                "# Train model\n",
                "print(\"\\nStarting training (this may take 15-30 minutes on GPU)...\")\n",
                "trainer.train(\n",
                "    X_train_final, y_train,\n",
                "    X_val_final, y_val,\n",
                "    verbose=True\n",
                ")\n",
                "\n",
                "print(\"\\n[OK] Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "download_header"
            },
            "source": [
                "## Step 7: Evaluate Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "zip_model"
            },
            "outputs": [],
            "source": [
                "# Create save directory\n",
                "save_path = PATHS['save_dir'] / 'baseline'\n",
                "save_path.mkdir(parents=True, exist_ok=True)\n",
                "eval_dir = save_path / 'evaluation'\n",
                "\n",
                "# Initialize evaluator\n",
                "evaluator = ModelEvaluator(save_dir=str(eval_dir))\n",
                "\n",
                "# Run baseline evaluation\n",
                "print(\"Running baseline evaluation...\")\n",
                "baseline_metrics = evaluator.evaluate_baseline(trainer, X_val_final, y_val)\n",
                "\n",
                "# Display results\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"BASELINE PERFORMANCE\")\n",
                "print(\"=\"*60)\n",
                "for target in TARGET_COLUMNS:\n",
                "    if target in baseline_metrics:\n",
                "        m = baseline_metrics[target]\n",
                "        print(f\"\\n{target}:\")\n",
                "        print(f\"  MAE:  {m['mae']:.4f}\")\n",
                "        print(f\"  RMSE: {m['rmse']:.4f}\")\n",
                "        print(f\"  R2:   {m['r2']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download_model"
            },
            "outputs": [],
            "source": [
                "# Test robustness with missing values\n",
                "print(\"Testing robustness with missing values...\")\n",
                "robustness_results = evaluator.test_missing_value_robustness(\n",
                "    trainer,\n",
                "    preprocessor,\n",
                "    X_val_raw,\n",
                "    y_val,\n",
                "    missing_levels=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
                "    n_trials=3\n",
                ")\n",
                "\n",
                "# Generate full report\n",
                "evaluator.generate_report()\n",
                "print(\"\\n[OK] Evaluation complete! Report saved to:\", eval_dir)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "usage_header"
            },
            "source": [
                "## Step 8: Visualize Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_predict"
            },
            "outputs": [],
            "source": [
                "from IPython.display import Image, display\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Display robustness curves if generated\n",
                "plot_path = eval_dir / 'robustness_curves.png'\n",
                "if plot_path.exists():\n",
                "    display(Image(filename=str(plot_path)))\n",
                "else:\n",
                "    # Plot manually from results\n",
                "    import pandas as pd\n",
                "    \n",
                "    df = pd.DataFrame(robustness_results)\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "    \n",
                "    # R2 vs Missing %\n",
                "    axes[0].plot(df['missing_pct'] * 100, df['carbon_total_r2'], 'b-o', label='Carbon Total')\n",
                "    axes[0].plot(df['missing_pct'] * 100, df['water_total_r2'], 'g-o', label='Water Total')\n",
                "    axes[0].set_xlabel('Missing Data (%)')\n",
                "    axes[0].set_ylabel('R2 Score')\n",
                "    axes[0].set_title('Model Robustness: R2 vs Missing Data')\n",
                "    axes[0].legend()\n",
                "    axes[0].grid(True, alpha=0.3)\n",
                "    \n",
                "    # MAE vs Missing %\n",
                "    axes[1].plot(df['missing_pct'] * 100, df['carbon_total_mae'], 'b-o', label='Carbon Total')\n",
                "    axes[1].plot(df['missing_pct'] * 100, df['water_total_mae'], 'g-o', label='Water Total')\n",
                "    axes[1].set_xlabel('Missing Data (%)')\n",
                "    axes[1].set_ylabel('MAE')\n",
                "    axes[1].set_title('Model Robustness: MAE vs Missing Data')\n",
                "    axes[1].legend()\n",
                "    axes[1].grid(True, alpha=0.3)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "next_steps"
            },
            "source": [
                "## Step 9: Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save trained model and preprocessor\n",
                "print(f\"Saving model to: {save_path}\")\n",
                "\n",
                "trainer.save(str(save_path))\n",
                "preprocessor.save(str(save_path / 'preprocessor.pkl'))\n",
                "\n",
                "print(\"\\nSaved files:\")\n",
                "for f in save_path.iterdir():\n",
                "    size = f.stat().st_size / 1024\n",
                "    print(f\"  {f.name}: {size:.1f} KB\")\n",
                "\n",
                "print(\"\\n[OK] Model saved successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Done!\n",
                "\n",
                "Training complete. Your model is saved locally at:\n",
                "- `models/saved/gpu_training/baseline/`\n",
                "\n",
                "### Model Files:\n",
                "- `xgb_model.json` - XGBoost model weights\n",
                "- `trainer_config.pkl` - Training configuration  \n",
                "- `preprocessor.pkl` - Fitted preprocessor\n",
                "- `evaluation/` - Performance metrics and plots\n",
                "\n",
                "### Expected Performance:\n",
                "- **R2 > 0.90** for all targets\n",
                "- **MAE < 0.10 kg CO2e** for carbon predictions\n",
                "- Graceful degradation with up to 30% missing data"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
